---
---

@inproceedings{hu20icml,
    abbr = "ICML",
    code = "https://github.com/google-research/xtreme",
    selected = "1", 
    title = {{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation},
    author = {Junjie Hu and Sebastian Ruder and Aditya Siddhant and Graham Neubig and Orhan Firat and Melvin Johnson},
    booktitle = {International Conference on Machine Learning (ICML)},
    month = {July},
    url = {https://arxiv.org/pdf/2003.11080.pdf},
    year = {2020}
}

@inproceedings{han20icml,
    abbr = "ICML",
    title = {On Learning Language-Invariant Representations for Universal Machine Translation},
    author = {Zhao, Han and Hu, Junjie and Risteski,  Andrej},
    booktitle = {International Conference on Machine Learning (ICML)},
    month = {July},
    url = {},
    year = {2020}
}


@inproceedings{huang-etal-2020-unsupervised,
    abbr={ACL},
    title = "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting",
    author = "Huang, Po-Yao  and
      Hu, Junjie  and
      Chang, Xiaojun  and
      Hauptmann, Alexander",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.731",
    pages = "8226--8237",
    abstract = "Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.",
}

@inproceedings{hu20aaai,
    abbr = "AAAI",
    code = "https://github.com/JunjieHu/ReCo-RL",
    title = {What Makes A Good Story? Designing Composite Rewards for Visual Storytelling},
    author = {Junjie Hu and Yu Cheng and Zhe Gan and Jingjing Liu and Jianfeng Gao and Graham Neubig},
    booktitle = {Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)},
    address = {New York, USA},
    month = {February},
    url = {https://arxiv.org/abs/1909.05316},
    year = {2020}
}

@inproceedings{hu-etal-2019-domain,
    abbr={ACL},
    code = "https://github.com/junjiehu/dali",
    title = "Domain Adaptation of Neural Machine Translation by Lexicon Induction",
    author = "Hu, Junjie  and
      Xia, Mengzhou  and
      Neubig, Graham  and
      Carbonell, Jaime",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1286",
    doi = "10.18653/v1/P19-1286",
    pages = "2989--3001",
    abstract = "It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",
}

@inproceedings{yang2019hybrid,
  abbr = "CIKM",
  code = "https://github.com/yangliuy/HybridNCM",
  title={A hybrid retrieval-generation neural conversation model},
  author={Yang, Liu and Hu, Junjie and Qiu, Minghui and Qu, Chen and Gao, Jianfeng and Croft, W Bruce and Liu, Xiaodong and Shen, Yelong and Liu, Jingjing},
  booktitle={Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
  pages={1341--1350},
  year={2019},
  url = "https://arxiv.org/pdf/1904.09068.pdf",
}

@inproceedings{jiang-etal-2019-reo,
    abbr = "EMNLP",
    title = "{REO}-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning",
    author = "Jiang, Ming  and
      Hu, Junjie  and
      Huang, Qiuyuan  and
      Zhang, Lei  and
      Diesner, Jana  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1156",
    doi = "10.18653/v1/D19-1156",
    pages = "1475--1480",
    abstract = "Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the system{'}s overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.",
}

@inproceedings{zhou-etal-2019-handling,
    abbr = "EMNLP",
    title = "Handling Syntactic Divergence in Low-resource Machine Translation",
    author = "Zhou, Chunting  and
      Ma, Xuezhe  and
      Hu, Junjie  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1143",
    doi = "10.18653/v1/D19-1143",
    pages = "1388--1394",
    abstract = "Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.",
}

@inproceedings{dou-etal-2019-unsupervised,
    abbr = "EMNLP",
    title = "Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings",
    author = "Dou, Zi-Yi  and
      Hu, Junjie  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1147",
    doi = "10.18653/v1/D19-1147",
    pages = "1417--1422",
    abstract = "The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.",
}


@inproceedings{dou-etal-2019-domain,
    abbr = "WNGT",
    title = "Domain Differential Adaptation for Neural Machine Translation",
    author = "Dou, Zi-Yi  and
      Wang, Xinyi  and
      Hu, Junjie  and
      Neubig, Graham",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5606",
    doi = "10.18653/v1/D19-5606",
    pages = "59--69",
    abstract = "Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of \textit{Domain Differential Adaptation (DDA)}, where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.",
}

@inproceedings{neubig-hu-2018-rapid,
    abbr = "EMNLP",
    code = "https://github.com/neubig/rapid-adaptation",
    title = "Rapid Adaptation of Neural Machine Translation to New Languages",
    author = "Neubig, Graham  and  Hu, Junjie",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1103",
    doi = "10.18653/v1/D18-1103",
    pages = "875--880",
    abstract = "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual {``}seed models{''}, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of {``}similar-language regularization{''}, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",
}

@inproceedings{neubig-etal-2019-compare,
    abbr = "NAACL",
    code = "https://github.com/neulab/compare-mt",
    video = "https://youtu.be/NyJEQT7t2CA",
    title = "compare-mt: A Tool for Holistic Comparison of Language Generation Systems",
    author = "Neubig, Graham  and
      Dou, Zi-Yi  and
      Hu, Junjie  and
      Michel, Paul  and
      Pruthi, Danish  and
      Wang, Xinyi",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-4007",
    doi = "10.18653/v1/N19-4007",
    pages = "35--41",
    abstract = "In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation. The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement. It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system. It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis. compare-mt is a pure-Python open source package, that has already proven useful to generate analyses that have been used in our published papers. Demo Video: https://youtu.be/NyJEQT7t2CA",
}

@inproceedings{stewart-etal-2018-automatic,
    abbr = "ACL",
    title = "Automatic Estimation of Simultaneous Interpreter Performance",
    author = "Stewart, Craig  and
      Vogler, Nikolai  and
      Hu, Junjie  and
      Boyd-Graber, Jordan  and
      Neubig, Graham",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2105",
    doi = "10.18653/v1/P18-2105",
    pages = "662--666",
    abstract = "Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding. Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computer-assisted interpretation interfaces or pedagogical tools. We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output. In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.",
}


@inproceedings{hu-etal-2018-contextual,
    abbr = "WMT",
    code = "https://github.com/junjiehu/ceqe",
    title = "Contextual Encoding for Translation Quality Estimation",
    author = "Hu, Junjie  and
      Chang, Wei-Cheng  and
      Wu, Yuexin  and
      Neubig, Graham",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6462",
    doi = "10.18653/v1/W18-6462",
    pages = "788--793",
    abstract = "The task of word-level quality estimation (QE) consists of taking a source sentence and machine-generated translation, and predicting which words in the output are correct and which are wrong. In this paper, propose a method to effectively encode the local and global contextual information for each target word using a three-part neural network approach. The first part uses an embedding layer to represent words and their part-of-speech tags in both languages. The second part leverages a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.",
}

@inproceedings{liu-etal-2017-structural,
    abbr = "EMNLP",
    title = "Structural Embedding of Syntactic Trees for Machine Comprehension",
    author = "Liu, Rui  and
      Hu, Junjie  and
      Wei, Wei  and
      Yang, Zi  and
      Nyberg, Eric",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1085",
    doi = "10.18653/v1/D17-1085",
    pages = "815--824",
    abstract = "Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.",
}

@inproceedings{yang-etal-2017-semi,
    abbr = "ACL",
    title = "Semi-Supervised {QA} with Generative Domain-Adaptive Nets",
    author = "Yang, Zhilin  and
      Hu, Junjie  and
      Salakhutdinov, Ruslan  and
      Cohen, William",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1096",
    doi = "10.18653/v1/P17-1096",
    pages = "1040--1050",
    abstract = "We study the problem of semi-supervised question answering{---}utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the \textit{Generative Domain-Adaptive Nets}. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",
}

@inproceedings{hu2017answer,
  abbr = "AAAI",
  url = "https://www.ttic.edu/nchrc/papers/27.pdf",
  title={Answer-aware attention on grounded question answering in images},
  author={Hu, Junjie and Fan, Desai and Yao, Shuxin and Oh, Jean},
  year={2017},
  booktitle = "AAAI 2017 Fall Symposium on Natural Communication for Human-Robot Collaboration",
}

@article{6971101,
  abbr = "IEEE Cybern.",
  author={Ng, Wing W. Y.  and Hu, Junjie and Yeung, Daniel Yeung and Yin, Shaohua and Roli, Fabio},
  journal={IEEE Transactions on Cybernetics},
  title={Diversified Sensitivity-Based Undersampling for Imbalance Classification Problems},
  year={2015},
  volume={45},
  number={11},
  pages={2402-2412},
  abstract={Undersampling is a widely adopted method to deal with imbalance pattern classification problems. Current methods mainly depend on either random resampling on the majority class or resampling at the decision boundary. Random-based undersampling fails to take into consideration informative samples in the data while resampling at the decision boundary is sensitive to class overlapping. Both techniques ignore the distribution information of the training dataset. In this paper, we propose a diversified sensitivity-based undersampling method. Samples of the majority class are clustered to capture the distribution information and enhance the diversity of the resampling. A stochastic sensitivity measure is applied to select samples from both clusters of the majority class and the minority class. By iteratively clustering and sampling, a balanced set of samples yielding high classifier sensitivity is selected. The proposed method yields a good generalization capability for 14 UCI datasets.},
  url={https://ieeexplore.ieee.org/abstract/document/6971101},
}

@article{hu2017online,
  abbr={IEEE TNNLS},
  title={Online nonlinear AUC maximization for imbalanced data sets},
  author={Hu, Junjie and Yang, Haiqin and Lyu, Michael R and King, Irwin and So, Anthony Man-Cho},
  journal={IEEE transactions on neural networks and learning systems},
  volume={29},
  number={4},
  pages={882--895},
  year={2017},
  publisher={IEEE},
  abstract={Classifying binary imbalanced streaming data is a significant task in both machine learning and data mining. Previously, online area under the receiver operating characteristic (ROC) curve (AUC) maximization has been proposed to seek a linear classifier. However, it is not well suited for handling nonlinearity and heterogeneity of the data. In this paper, we propose the kernelized online imbalanced learning (KOIL) algorithm, which produces a nonlinear classifier for the data by maximizing the AUC score while minimizing a functional regularizer. We address four major challenges that arise from our approach. First, to control the number of support vectors without sacrificing the model performance, we introduce two buffers with fixed budgets to capture the global information on the decision boundary by storing the corresponding learned support vectors. Second, to restrict the fluctuation of the learned decision function and achieve smooth updating, we confine the influence on a new support vector to its k-nearest opposite support vectors. Third, to avoid information loss, we propose an effective compensation scheme after the replacement is conducted when either buffer is full. With such a compensation scheme, the performance of the learned model is comparable to the one learned with infinite budgets. Fourth, to determine good kernels for data similarity representation, we exploit the multiple kernel learning framework to automatically learn a set of kernels. Extensive experiments on both synthetic and real-world benchmark data sets demonstrate the efficacy of our proposed approach.},
  url={https://ieeexplore.ieee.org/document/7835710},
}

@inproceedings{AAAI159578,
    abbr={AAAI},
    author = {Junjie Hu and Haiqin Yang and Irwin King and Michael Lyu and Anthony Man-Cho So},
    title = {Kernelized Online Imbalanced Learning with Fixed Budgets},
    booktitle = {Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)},
    year = {2015},
    keywords = {},
    abstract = {Online learning from imbalanced streaming data to capture the nonlinearity and heterogeneity of the data is significant in machine learning and data mining.  To tackle this problem, we propose a kernelized online imbalanced learning (KOIL) algorithm to directly maximize the area under the ROC curve (AUC).  We address two more challenges: 1) How to control the number of support vectors without sacrificing model performance; and 2) how to restrict the fluctuation of the learned decision function to attain smooth updating.  To this end, we introduce two buffers with fixed budgets (buffer sizes) for positive class and negative class, respectively, to store the learned support vectors, which can allow us to capture the global information of the decision boundary.  When determining the weight of a new support vector, we confine its influence only to its $k$-nearest opposite support vectors.  This can restrict the effect of new instances and prevent the harm of outliers.  More importantly, we design a sophisticated scheme to compensate the model after replacement is conducted when either buffer is full.  With this compensation, the learned model approaches the one learned with infinite budgets.  We present both theoretical analysis and extensive experimental comparison to demonstrate the effectiveness of our proposed KOIL.},
    url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9578}
}

@inproceedings{gao2015ar,
  abbr={SOSE},
  title={Ar-tracker: Track the dynamics of mobile apps via user review mining},
  author={Gao, Cuiyun and Xu, Hui and Hu, Junjie and Zhou, Yangfan},
  booktitle={2015 IEEE Symposium on Service-Oriented System Engineering},
  pages={284--290},
  year={2015},
  organization={IEEE},
  abstract={User-generated reviews on mobile applications (apps) are a valuable source of data for developers to improve the quality of their apps. But the reviews are usually massive in size and span over multiple topics, thus leading to great challenges for developers to efficiently identify the key reviews of interest. In recent studies, automatic user review mining has been recognized as a key solution to address this challenge. The existing methods, however, require extensive human efforts to manually label the training data. Besides, they only analyze the static characteristics over the whole set of collected reviews, while ignoring the dynamic information embedded in the reviews of different time periods. In this paper, we propose 'AR-Tracker', a new framework to mine user reviews without the need of human labeling and track the dynamics from the top-ranked reviews. Through extensive experiments on the reviews of four popular mobile apps collected over 7 months, we show that AR-Tracker can still achieve comparable accuracy with the state-of-the-art methods, e.g., AR-Miner. Additionally, a case study on Facebook reviews further validates the effectiveness of 'AR-Tracker' in tracking the dynamics.},
  url={https://ieeexplore.ieee.org/abstract/document/7133542},
}

@inproceedings{huHCOMP16,
  abbr={HCOMP},
  author = {Junjie Hu and Jean Oh and Anatole Gershman},
  title = {Learning Lexical Entries for Robotic Commands via Paraphrasing},
  booktitle = {AAAI conference on Human Computation},
  year = {2016},
  url = {https://arxiv.org/pdf/1609.02549.pdf},
  abstract = {Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.},
}

@inproceedings{yangICLR16,
  abbr={ICLR},
  author = {Zhilin Yang and Bhuwan Dhingra and Ye Yuan and Junjie Hu and William W. Cohen and Ruslan Salakhutdinov.},
  title = {Words or Characters? Fine-grained Gating for Reading Comprehension},
  booktitle = {International Conference on Learning Representations},
  year = {2016},
  url = {https://arxiv.org/pdf/1611.01724.pdf},
  abstract = {Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children's Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.},
}




