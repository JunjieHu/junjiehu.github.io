<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Junjie Hu
 | Publications</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

<!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
<!--       
        
        <a class="navbar-brand title font-weight-lighter" href="http://localhost:4001/"><span class="font-weight-bold">Junjie</span> Hu</a>
       -->
    
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <!-- <a class="scroll-top navbar-brand" href="http://www.cs.cmu.edu/~junjieh/"><b>Junjie Hu</b></a> -->

      <div id="navbarNav" class="collapse navbar-collapse">
        <!-- <ul class="navbar-nav ml-auto flex-nowrap"> -->
        <ul class="nav navbar-nav">

          
            <li><a href="#group">Group</a></li>
            <li><a href="#publications">Publications</a></li>
            <li><a href="#teaching">Teaching</a></li>
            <li><a href="#talks">Talks</a></li>
            <li><a href="#awards">Awards</a></li>

          
        </ul>
        
<!-- <span class="contact-icon text-center"> -->
<!-- <ul class="nav navbar-nav navbar-right">
  <a href="mailto:%6A%75%6E%6A%69%65%68@%63%73.%63%6D%75.%65%64%75
"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=j-42gHYAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/JunjieHu" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/junjie-hu-24b48b83" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/JunjieHu12" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
</ul>  -->
<!-- </span> -->


<ul class="nav navbar-nav" id="social">
  <li><a href="https://github.com/JunjieHu"><i class="fab fa-github fa-1x"></i></a></li>
  <li><a href="https://www.linkedin.com/in/junjie-hu-24b48b83"><i class="fab fa-linkedin fa-1x"></i></a></li>
  <li><a href="https://scholar.google.com/citations?user=j-42gHYAAAAJ"><i class="fab fa-google fa-1x"></i></a></li>
  <li><a href="https://www.facebook.com/JunjieHuu"><i class="fab fa-facebook fa-1x"></i></a></li>
  <li><a href="https://twitter.com/JunjieHu12"><i class="fab fa-twitter fa-1x"></i></a></li>
  <li class="divider vertical"></li>
</ul>
      </div>

    </div>
  </nav>
    


</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">Publications by categories in reversed chronological order.</p>
  </header>

  <article>
    
<!-- <div class="info-list"> -->
<!-- <h3 id="publications" name="publications" style="display:inline">Publications

    <div class="onoffswitch">
    <input type="checkbox" onclick="onClickHandler();" name="onoffswitch" class="onoffswitch-checkbox" id="myonoffswitch" tabindex="0" checked>
    <label class="onoffswitch-label" for="myonoffswitch">
        <span class="onoffswitch-inner"></span>
        <span class="onoffswitch-switch"></span>
    </label>
    </div>
</h3> -->
<p><!-- [
    <a href="" id="pub-by-selected" style="text-decoration: underline; color: rgb(0, 0, 0);">show
        selected</a>
    /
    <a href="" id="pub-by-date">show by date</a>
    ] --></p>
<h3 id="publications" name="publications">Recent Preprints</h3>
<div class="publications" id="preprint-list-by-date" style="display:inline">

</div>

<h3>Publications </h3>
<p>&lt;/br&gt;</p>
<div class="publications" id="pub-list-by-date" style="display:inline">

</div>

<div class="publications" id="pub-list-by-selected" style="display:none">
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="reid21emnlp" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2109.04715">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages</a></span>
      <span class="author">
        
          
            
              
                
                  Machel Reid,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Yutaka Matsuo
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/machelreid/afromt" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="ruder21emnlp" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2104.07412">XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation</a></span>
      <span class="author">
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Noah Constant,
                
              
            
          
        
          
            
              
                
                  Jan Botha,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  Jinlan Fu,
                
              
            
          
        
          
            
              
                
                  Pengfei Liu,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Dan Garrette,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Melvin Johnson
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/google-research/xtreme" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="hu21explicit" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2010.07972.pdf">Explicit Alignment Objectives for Multilingual Bidirectional Encoders</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Melvin Johnson,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="http://github.com/junjiehu/amber" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="hu20icml" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2003.11080.pdf">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  and Melvin Johnson
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In International Conference on Machine Learning (ICML)</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/google-research/xtreme" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.</p>
    </span>
    
  </div>
</div>
</li></ol>
</div>


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Junjie Hu
.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>. 

    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







<script type="text/javascript">
  function display(id) {
    var currentDisplay = document.getElementById(id).style.display;
    if (currentDisplay == 'none') {
        document.getElementById(id).style.display = "inline";
    }
    else {
        document.getElementById(id).style.display = "none";
    }
  }

  function onClickHandler(){
    display('pub-list-by-selected');
    display('pub-list-by-date')
  }
    // $("#pub-by-selected").on('click', function() {
    //   document.getElementById('pub-list-by-date').style.display = "none";
    //   document.getElementById('pub-list-by-selected').style.display = "inline";
    //   $('#pub-by-data').attr("style", "");
    //   $('#pub-by-selected').css({ "color": "#000000", "text-decoration": "underline" });
    // }

    // $("#pub-by-date").on('click', function() {
    //   document.getElementById('pub-list-by-date').style.display = "inline";
    //   document.getElementById('pub-list-by-selected').style.display = "none";
    //   $('#pub-by-selected').attr("style", "");
    //   $('#pub-by-date').css({ "color": "#000000", "text-decoration": "underline" });
    // }
</script>

</html>
