<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Junjie Hu
</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

<!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
<!--       
        
        <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000/"><span class="font-weight-bold">Junjie</span> Hu</a>
       -->
    
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <!-- <a class="scroll-top navbar-brand" href="http://www.cs.cmu.edu/~junjieh/"><b>Junjie Hu</b></a> -->

      <div id="navbarNav" class="collapse navbar-collapse">
        <!-- <ul class="navbar-nav ml-auto flex-nowrap"> -->
        <ul class="nav navbar-nav">

          
            <li><a href="#group">Group</a></li>
            <li><a href="#publications">Publications</a></li>
            <li><a href="#teaching">Teaching</a></li>
            <li><a href="#talks">Talks</a></li>
            <li><a href="#awards">Awards</a></li>

          
        </ul>
        
<!-- <span class="contact-icon text-center"> -->
<!-- <ul class="nav navbar-nav navbar-right">
  <a href="mailto:%6A%75%6E%6A%69%65%68@%63%73.%63%6D%75.%65%64%75
"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=j-42gHYAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/JunjieHu" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/junjie-hu-24b48b83" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/JunjieHu12" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
</ul>  -->
<!-- </span> -->


<ul class="nav navbar-nav" id="social">
  <li><a href="https://github.com/JunjieHu"><i class="fab fa-github fa-1x"></i></a></li>
  <li><a href="https://www.linkedin.com/in/junjie-hu-24b48b83"><i class="fab fa-linkedin fa-1x"></i></a></li>
  <li><a href="https://scholar.google.com/citations?user=j-42gHYAAAAJ"><i class="fab fa-google fa-1x"></i></a></li>
  <li><a href="https://www.facebook.com/JunjieHuu"><i class="fab fa-facebook fa-1x"></i></a></li>
  <li><a href="https://twitter.com/JunjieHu12"><i class="fab fa-twitter fa-1x"></i></a></li>
  <li class="divider vertical"></li>
</ul>
      </div>

    </div>
  </nav>
    


</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    
    <div class="container">
        <div class="row">
            <div class="col-md-8">
                <h1 class="post-title"><span class="font-weight-bold">Junjie Hu | 胡俊杰 </span> </h1>
                <address>
                    <br> Assistant Professor <br> <a href="https://www.biostat.wisc.edu/">Biostatistics & Medical Informatics </a>  <br> <a href="https://www.cs.wisc.edu/">Computer Science </a>  <br> <a href="https://www.wisc.edu/">University of Wisconsin-Madison </a>  <br> Office: 4735 MSC, 1300 University Ave, Madison, WI 53706 <br> Email: junjie.hu@wisc.edu <br> [<a href="assets/pdf/ResearchStatement_JunjieHu.pdf">Research Statement</a>]

                </address>
            </div>
            <div class="col-md-4 profile">
                <br>
                <img src="/assets/img/junjiehu.png">
            </div>
        </div>
    </div>

  </header>

  <article>

<!--     <span id="bio" name="bio">
        
    </span> -->
    <div class="info-list">
      <h3>About</h3>
    </div>
    <table class="table table-hover">
        <td>
            <p>I am an assistant professor in the Department of Biostatistics and Department of Computer Science at the University of Wisconsin-Madison. I obtained my Ph.D. from School of Computer Science at Carnegie Mellon University, where I worked with Jaime Carbonell and Graham Neubig.</p>

<p>I have a broad interest in <b>natural language processing</b> and <b>machine learning</b>. In particular, I work on multilingual NLP, transfer learning, multimodal learning, and their applications to support human-machine communications. My research goal is to build robust intelligent systems that evolve with changes in the environment and interact with people speaking different languages.</p>

<p><b>Prospective students</b>: Thanks for your interest! I am always looking for excellent PhD students to join our lab. Please apply to the <a href="https://www.cs.wisc.edu/graduate/graduate-admissions-faq/">CS</a> or <a href="https://biostat.wiscweb.wisc.edu/education/current-students/phd-bds/">BDS</a> program, and mention my name in your application and research statement. UW-Madison is an excellent place for research, and Madison is a wonderful city to live in. Please check out these videos (Why <a href="https://www.youtube.com/watch?v=8cRE4F8GOBE">UW-Madison</a>, <a href="https://www.youtube.com/watch?v=XTJA5alrisQ?">Madison</a>). I’m also happy to work with masters or undergraduate students at UW-Madison. If you are interested, please send me an email.</p>

        </td>
    </table>


    

    
      
<div class="info-list">
  <h3 id="group" name="group">Research Group</h3>
      <table class="table table-hover" style="font-size:80%">
        <td>
            <p>I am really fortunate to work with a group of excellent students at UW-Madison. Stay tuned on our latest works! </p>


  <!-- <p>I am fortunate to work with a group of excellent students at UW-Madison. Stay tuned on our latest works! </p> -->
  Graduate Students
  <ul>
    <li><p><a href="https://uppaal.github.io/">Rheeya Uppaal</a> (PhD in CS) </p></li>
    <li><p><a href="https://www.linkedin.com/in/gowtham1/?originalSubdomain=in">Gowtham Ramesh</a> (PhD in CS) </p></li>
    <li><p><a href="https://www.linkedin.com/in/tim-ossowski-bb07a0171/">Tim Ossowski</a> (PhD in CS) </p></li>
    <li><p><a href="https://scholar.google.com/citations?user=1BGJ0YMAAAAJ&hl=en">Makesh Narsimhan Sreedhar</a> (Research MS in CS) </p></li>
    <li><p><a href="https://www.linkedin.com/in/samarth-mathur1/">Samarth Mathur</a> (Research MS in ECE) </p></li>
    <li><p><a href="https://github.com/Aurora-He">Shilu He</a> (MS in Math) </p></li>
  </ul>
  Undergraduate Students
  <ul>
    <li><p><a href="https://www.linkedin.com/in/shanchao-liang-106480195/">Shanchao Liang</a> (BA in CS/Math, next PhD in CS at Purdue) </p></li>
    <li><p><a href="https://www.linkedin.com/in/jackcai1206/">Jack Ziyang Cai</a> (BA in CS/Math) </p></li>
    <li><p><a href="https://github.com/sleepope">Shunchi Zhang</a> (Exchange student from XJTU) </p></li>
    <li><p>Jiayuan Rao (Exchange student from SJTU) </p></li>
  </ul>
          </td>
    </table>
</div>


      
<!-- <div class="info-list"> -->
<!-- <h3 id="publications" name="publications" style="display:inline">Publications

    <div class="onoffswitch">
    <input type="checkbox" onclick="onClickHandler();" name="onoffswitch" class="onoffswitch-checkbox" id="myonoffswitch" tabindex="0" checked>
    <label class="onoffswitch-label" for="myonoffswitch">
        <span class="onoffswitch-inner"></span>
        <span class="onoffswitch-switch"></span>
    </label>
    </div>
</h3> -->
  <!-- [
    <a href="" id="pub-by-selected" style="text-decoration: underline; color: rgb(0, 0, 0);">show
        selected</a>
    /
    <a href="" id="pub-by-date">show by date</a>
    ] -->
<h3 id="publications" name="publications">Publications </h3>

<!--  <div class="info-list">
  <h3 id="publications" name="publications">Publications
  <div class="onoffswitch">
  <input type="checkbox" onclick="onClickHandler();" name="onoffswitch" class="onoffswitch-checkbox" id="myonoffswitch" tabindex="0" checked>
  <label class="onoffswitch-label" for="myonoffswitch">
      <span class="onoffswitch-inner"></span>
      <span class="onoffswitch-switch"></span>
  </label>
  </div>
</div>
<p style="top-margin: 110px;"></p> -->

<!-- </div> -->

<div class="publications" id="pub-list-by-date" style="display:inline">

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE TPAMI</abbr>
    
  
  </div>

  <div id="li2022vpumn" class="col-sm-8">
    
      <span class="title"><a href="">Video Pivoting Unsupervised Multi-modal Neural
Machine Translation</a></span>
      <span class="author">
        
          
            
              
                
                  Mingjie Li,
                
              
            
          
        
          
            
              
                
                  Po-Yao Huang,
                
              
            
          
        
          
            
              
                
                  Xiaojun Chang,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Yi Yang,
                
              
            
          
        
          
            
              
                
                  and Alex Hauptmann
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE transactions on pattern analysis and machine intelligence (To Appear)</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="hu2022Deep" class="col-sm-8">
    
      <span class="title"><a href="https://aclanthology.org/2022.acl-long.123.pdf">DEEP: DEnoising Entity Pre-training for Neural Machine Translation</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Hiroaki Hayashi,
                
              
            
          
        
          
            
              
                
                  Kyunghyun Cho,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="Ding2021GlobalWoZGM" class="col-sm-8">
    
      <span class="title"><a href="https://aclanthology.org/2022.acl-long.115.pdf">GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems</a></span>
      <span class="author">
        
          
            
              
                
                  Bosheng Ding,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Lidong Bing,
                
              
            
          
        
          
            
              
                
                  Sharifah Aljunied Mahani,
                
              
            
          
        
          
            
              
                
                  Shafiq R. Joty,
                
              
            
          
        
          
            
              
                
                  Luo Si,
                
              
            
          
        
          
            
              
                
                  and Chunyan Miao
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WMT</abbr>
    
  
  </div>

  <div id="hu21wmt" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2106.11375">Phrase-level Active Learning for Neural Machine Translation</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In The Sixth Conference on Machine Translation (WMT)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/JunjieHu/phrase-al-nmt" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Neural machine translation (NMT) is sensitive to domain shift. In this paper, we address this problem in an active learning setting where we can spend a given budget on translating in-domain data, and gradually fine-tune a pre-trained out-of-domain NMT model on the newly translated data. Existing active learning methods for NMT usually select sentences based on uncertainty scores, but these methods require costly translation of full sentences even when only one or two key phrases within the sentence are informative. To address this limitation, we re-examine previous work from the phrase-based machine translation (PBMT) era that selected not full sentences, but rather individual phrases. However, while incorporating these phrases into PBMT systems was relatively simple, it is less trivial for NMT systems, which need to be trained on full sequences to capture larger structural properties of sentences unique to the new domain. To overcome these hurdles, we propose to select both full sentences and individual phrases from unlabelled data in the new domain for routing to human translators. In a German-English translation task, our active learning approach achieves consistent improvements over uncertainty-based sentence selection methods, improving up to 1.2 BLEU score over strong active learning baselines.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="reid21emnlp" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2109.04715">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages</a></span>
      <span class="author">
        
          
            
              
                
                  Machel Reid,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Yutaka Matsuo
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/machelreid/afromt" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="ruder21emnlp" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2104.07412">XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation</a></span>
      <span class="author">
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Noah Constant,
                
              
            
          
        
          
            
              
                
                  Jan Botha,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  Jinlan Fu,
                
              
            
          
        
          
            
              
                
                  Pengfei Liu,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Dan Garrette,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Melvin Johnson
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/google-research/xtreme" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="hu21explicit" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2010.07972.pdf">Explicit Alignment Objectives for Multilingual Bidirectional Encoders</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Melvin Johnson,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="http://github.com/junjiehu/amber" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="huang21visionlang" class="col-sm-8">
    
      <span class="title"><a href="https://aclanthology.org/2021.naacl-main.195.pdf">Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</a></span>
      <span class="author">
        
          
            
              
                
                  Po-Yao Huang,
                
              
            
          
        
          
            
              
                
                  Mandela Patrick,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Florian Metze,
                
              
            
          
        
          
            
              
                
                  and Alexander Hauptmann
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="http://github.com/berniebear/Multi-HT100M" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="hu20icml" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2003.11080.pdf">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  and Melvin Johnson
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In International Conference on Machine Learning (ICML)</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/google-research/xtreme" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="han20icml" class="col-sm-8">
    
      <span class="title"><a href="http://proceedings.mlr.press/v119/zhao20b/zhao20b.pdf">On Learning Language-Invariant Representations for Universal Machine Translation</a></span>
      <span class="author">
        
          
            
              
                
                  Han Zhao,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  and Andrej Risteski
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In International Conference on Machine Learning (ICML)</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The goal of universal machine translation is to learn to translate between any pair of languages, given a corpus of paired translated documents for <i>a small subset</i> of all pairs of languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent. In this paper, we formally prove certain impossibilities of this endeavour in general, as well as prove positive results in the presence of additional (but natural) structure of data. For the former, we derive a lower bound on the translation error in the many-to-many translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made. For the latter, we show that if the paired documents in the corpus follow a natural <i>encoder-decoder</i> generative process, we can expect a natural notion of “generalization”: a linear number of language pairs, rather than quadratic, suffices to learn a good representation. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity in terms of the total number of documents per language pair needed. We believe our theoretical insights and implications contribute to the future algorithmic design of universal machine translation.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="huang-etal-2020-unsupervised" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/2020.acl-main.731">Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting</a></span>
      <span class="author">
        
          
            
              
                
                  Po-Yao Huang,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Xiaojun Chang,
                
              
            
          
        
          
            
              
                
                  and Alexander Hauptmann
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only. However, it is still challenging to associate source-target sentences in the latent space. As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT). In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT. Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision. The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Workshop</abbr>
    
  
  </div>

  <div id="anastasopoulos-etal-2020-tico" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/2020.nlpcovid19-2.5">TICO-19: the Translation Initiative for COvid-19</a></span>
      <span class="author">
        
          
            
              
                
                  Antonios Anastasopoulos,
                
              
            
          
        
          
            
              
                
                  Alessandro Cattelan,
                
              
            
          
        
          
            
              
                
                  Zi-Yi Dou,
                
              
            
          
        
          
            
              
                
                  Marcello Federico,
                
              
            
          
        
          
            
              
                
                  Christian Federmann,
                
              
            
          
        
          
            
              
                
                  Dmitriy Genzel,
                
              
            
          
        
          
            
              
                
                  Franscisco Guzmán,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Macduff Hughes,
                
              
            
          
        
          
            
              
                
                  Philipp Koehn,
                
              
            
          
        
          
            
              
                
                  Rosie Lazar,
                
              
            
          
        
          
            
              
                
                  Will Lewis,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Mengmeng Niu,
                
              
            
          
        
          
            
              
                
                  Alp Öktem,
                
              
            
          
        
          
            
              
                
                  Eric Paquin,
                
              
            
          
        
          
            
              
                
                  Grace Tang,
                
              
            
          
        
          
            
              
                
                  and Sylwia Tur
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, ”pivot” languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="hu20aaai" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/1909.05316">What Makes A Good Story? Designing Composite Rewards for Visual Storytelling</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Yu Cheng,
                
              
            
          
        
          
            
              
                
                  Zhe Gan,
                
              
            
          
        
          
            
              
                
                  Jingjing Liu,
                
              
            
          
        
          
            
              
                
                  Jianfeng Gao,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
      [<a href="https://github.com/JunjieHu/ReCo-RL" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="hu-etal-2019-domain" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/P19-1286">Domain Adaptation of Neural Machine Translation by Lexicon Induction</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Mengzhou Xia,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Jaime Carbonell
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/junjiehu/dali" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">CIKM</abbr>
    
  
  </div>

  <div id="yang2019hybrid" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/1904.09068.pdf">A hybrid retrieval-generation neural conversation model</a></span>
      <span class="author">
        
          
            
              
                
                  Liu Yang,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Minghui Qiu,
                
              
            
          
        
          
            
              
                
                  Chen Qu,
                
              
            
          
        
          
            
              
                
                  Jianfeng Gao,
                
              
            
          
        
          
            
              
                
                  W Bruce Croft,
                
              
            
          
        
          
            
              
                
                  Xiaodong Liu,
                
              
            
          
        
          
            
              
                
                  Yelong Shen,
                
              
            
          
        
          
            
              
                
                  and Jingjing Liu
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
      [<a href="https://github.com/yangliuy/HybridNCM" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="jiang-etal-2019-reo" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/D19-1156">REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning</a></span>
      <span class="author">
        
          
            
              
                
                  Ming Jiang,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Qiuyuan Huang,
                
              
            
          
        
          
            
              
                
                  Lei Zhang,
                
              
            
          
        
          
            
              
                
                  Jana Diesner,
                
              
            
          
        
          
            
              
                
                  and Jianfeng Gao
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Popular metrics used for evaluating image captioning systems, such as BLEU and CIDEr, provide a single score to gauge the system’s overall effectiveness. This score is often not informative enough to indicate what specific errors are made by a given system. In this study, we present a fine-grained evaluation method REO for automatically measuring the performance of image captioning systems. REO assesses the quality of captions from three perspectives: 1) Relevance to the ground truth, 2) Extraness of the content that is irrelevant to the ground truth, and 3) Omission of the elements in the images and human references. Experiments on three benchmark datasets demonstrate that our method achieves a higher consistency with human judgments and provides more intuitive evaluation results than alternative metrics.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="zhou-etal-2019-handling" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/D19-1143">Handling Syntactic Divergence in Low-resource Machine Translation</a></span>
      <span class="author">
        
          
            
              
                
                  Chunting Zhou,
                
              
            
          
        
          
            
              
                
                  Xuezhe Ma,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="dou-etal-2019-unsupervised" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/D19-1147">Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings</a></span>
      <span class="author">
        
          
            
              
                
                  Zi-Yi Dou,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Antonios Anastasopoulos,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WNGT</abbr>
    
  
  </div>

  <div id="dou-etal-2019-domain" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/D19-5606">Domain Differential Adaptation for Neural Machine Translation</a></span>
      <span class="author">
        
          
            
              
                
                  Zi-Yi Dou,
                
              
            
          
        
          
            
              
                
                  Xinyi Wang,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 3rd Workshop on Neural Generation and Translation</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of \textitDomain Differential Adaptation (DDA), where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="neubig-etal-2019-compare" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/N19-4007">compare-mt: A Tool for Holistic Comparison of Language Generation Systems</a></span>
      <span class="author">
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Zi-Yi Dou,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Paul Michel,
                
              
            
          
        
          
            
              
                
                  Danish Pruthi,
                
              
            
          
        
          
            
              
                
                  and Xinyi Wang
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/neulab/compare-mt" target="_blank">Code</a>]
    
    
      [<a style="color:#bb0000;"><strong>Best Demon Nomination</strong></a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation. The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement. It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system. It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis. compare-mt is a pure-Python open source package, that has already proven useful to generate analyses that have been used in our published papers. Demo Video: https://youtu.be/NyJEQT7t2CA</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="neubig-hu-2018-rapid" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/D18-1103">Rapid Adaptation of Neural Machine Translation to New Languages</a></span>
      <span class="author">
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                and <ins><strong>Junjie Hu</strong></ins>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/neubig/rapid-adaptation" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="stewart-etal-2018-automatic" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/P18-2105">Automatic Estimation of Simultaneous Interpreter Performance</a></span>
      <span class="author">
        
          
            
              
                
                  Craig Stewart,
                
              
            
          
        
          
            
              
                
                  Nikolai Vogler,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Jordan Boyd-Graber,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding. Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computer-assisted interpretation interfaces or pedagogical tools. We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output. In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">WMT</abbr>
    
  
  </div>

  <div id="hu-etal-2018-contextual" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/W18-6462">Contextual Encoding for Translation Quality Estimation</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Wei-Cheng Chang,
                
              
            
          
        
          
            
              
                
                  Yuexin Wu,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/junjiehu/ceqe" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The task of word-level quality estimation (QE) consists of taking a source sentence and machine-generated translation, and predicting which words in the output are correct and which are wrong. In this paper, propose a method to effectively encode the local and global contextual information for each target word using a three-part neural network approach. The first part uses an embedding layer to represent words and their part-of-speech tags in both languages. The second part leverages a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="liu-etal-2017-structural" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/D17-1085">Structural Embedding of Syntactic Trees for Machine Comprehension</a></span>
      <span class="author">
        
          
            
              
                
                  Rui Liu,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Wei Wei,
                
              
            
          
        
          
            
              
                
                  Zi Yang,
                
              
            
          
        
          
            
              
                
                  and Eric Nyberg
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Deep neural networks for machine comprehension typically utilizes only word or character embeddings without explicitly taking advantage of structured linguistic information such as constituency trees and dependency trees. In this paper, we propose structural embedding of syntactic trees (SEST), an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension. We evaluate our approach using a state-of-the-art neural attention model on the SQuAD dataset. Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers that are syntactically coherent over the baseline methods.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="yang-etal-2017-semi" class="col-sm-8">
    
      <span class="title"><a href="https://www.aclweb.org/anthology/P17-1096">Semi-Supervised QA with Generative Domain-Adaptive Nets</a></span>
      <span class="author">
        
          
            
              
                
                  Zhilin Yang,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Ruslan Salakhutdinov,
                
              
            
          
        
          
            
              
                
                  and William Cohen
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We study the problem of semi-supervised question answering—utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the \textitGenerative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="hu2017answer" class="col-sm-8">
    
      <span class="title"><a href="https://www.ttic.edu/nchrc/papers/27.pdf">Answer-aware attention on grounded question answering in images</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Desai Fan,
                
              
            
          
        
          
            
              
                
                  Shuxin Yao,
                
              
            
          
        
          
            
              
                
                  and Jean Oh
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In AAAI 2017 Fall Symposium on Natural Communication for Human-Robot Collaboration</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE TNNLS</abbr>
    
  
  </div>

  <div id="hu2017online" class="col-sm-8">
    
      <span class="title"><a href="https://ieeexplore.ieee.org/document/7835710">Online nonlinear AUC maximization for imbalanced data sets</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Haiqin Yang,
                
              
            
          
        
          
            
              
                
                  Michael R Lyu,
                
              
            
          
        
          
            
              
                
                  Irwin King,
                
              
            
          
        
          
            
              
                
                  and Anthony Man-Cho So
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE transactions on neural networks and learning systems</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Classifying binary imbalanced streaming data is a significant task in both machine learning and data mining. Previously, online area under the receiver operating characteristic (ROC) curve (AUC) maximization has been proposed to seek a linear classifier. However, it is not well suited for handling nonlinearity and heterogeneity of the data. In this paper, we propose the kernelized online imbalanced learning (KOIL) algorithm, which produces a nonlinear classifier for the data by maximizing the AUC score while minimizing a functional regularizer. We address four major challenges that arise from our approach. First, to control the number of support vectors without sacrificing the model performance, we introduce two buffers with fixed budgets to capture the global information on the decision boundary by storing the corresponding learned support vectors. Second, to restrict the fluctuation of the learned decision function and achieve smooth updating, we confine the influence on a new support vector to its k-nearest opposite support vectors. Third, to avoid information loss, we propose an effective compensation scheme after the replacement is conducted when either buffer is full. With such a compensation scheme, the performance of the learned model is comparable to the one learned with infinite budgets. Fourth, to determine good kernels for data similarity representation, we exploit the multiple kernel learning framework to automatically learn a set of kernels. Extensive experiments on both synthetic and real-world benchmark data sets demonstrate the efficacy of our proposed approach.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">HCOMP</abbr>
    
  
  </div>

  <div id="huHCOMP16" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/1609.02549.pdf">Learning Lexical Entries for Robotic Commands via Paraphrasing</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Jean Oh,
                
              
            
          
        
          
            
              
                
                  and Anatole Gershman
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In AAAI conference on Human Computation</em>
      
      
        2016
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="yangICLR16" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/1611.01724.pdf">Words or Characters? Fine-grained Gating for Reading Comprehension</a></span>
      <span class="author">
        
          
            
              
                
                  Zhilin Yang,
                
              
            
          
        
          
            
              
                
                  Bhuwan Dhingra,
                
              
            
          
        
          
            
              
                
                  Ye Yuan,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  William W. Cohen,
                
              
            
          
        
          
            
              
                
                  and Ruslan Salakhutdinov.
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In International Conference on Learning Representations</em>
      
      
        2016
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Previous work combines word-level and character-level representations using concatenation or scalar weighting, which is suboptimal for high-level tasks like reading comprehension. We present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the words. We also extend the idea of fine-grained gating to modeling the interaction between questions and paragraphs for reading comprehension. Experiments show that our approach can improve the performance on reading comprehension tasks, achieving new state-of-the-art results on the Children’s Book Test dataset. To demonstrate the generality of our gating mechanism, we also show improved results on a social media tag prediction task.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE Cybern.</abbr>
    
  
  </div>

  <div id="6971101" class="col-sm-8">
    
      <span class="title"><a href="https://ieeexplore.ieee.org/abstract/document/6971101">Diversified Sensitivity-Based Undersampling for Imbalance Classification Problems</a></span>
      <span class="author">
        
          
            
              
                
                  Wing W. Y. Ng,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Daniel Yeung Yeung,
                
              
            
          
        
          
            
              
                
                  Shaohua Yin,
                
              
            
          
        
          
            
              
                
                  and Fabio Roli
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE Transactions on Cybernetics</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Undersampling is a widely adopted method to deal with imbalance pattern classification problems. Current methods mainly depend on either random resampling on the majority class or resampling at the decision boundary. Random-based undersampling fails to take into consideration informative samples in the data while resampling at the decision boundary is sensitive to class overlapping. Both techniques ignore the distribution information of the training dataset. In this paper, we propose a diversified sensitivity-based undersampling method. Samples of the majority class are clustered to capture the distribution information and enhance the diversity of the resampling. A stochastic sensitivity measure is applied to select samples from both clusters of the majority class and the minority class. By iteratively clustering and sampling, a balanced set of samples yielding high classifier sensitivity is selected. The proposed method yields a good generalization capability for 14 UCI datasets.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="AAAI159578" class="col-sm-8">
    
      <span class="title"><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9578">Kernelized Online Imbalanced Learning with Fixed Budgets</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Haiqin Yang,
                
              
            
          
        
          
            
              
                
                  Irwin King,
                
              
            
          
        
          
            
              
                
                  Michael Lyu,
                
              
            
          
        
          
            
              
                
                  and Anthony Man-Cho So
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Online learning from imbalanced streaming data to capture the nonlinearity and heterogeneity of the data is significant in machine learning and data mining.  To tackle this problem, we propose a kernelized online imbalanced learning (KOIL) algorithm to directly maximize the area under the ROC curve (AUC).  We address two more challenges: 1) How to control the number of support vectors without sacrificing model performance; and 2) how to restrict the fluctuation of the learned decision function to attain smooth updating.  To this end, we introduce two buffers with fixed budgets (buffer sizes) for positive class and negative class, respectively, to store the learned support vectors, which can allow us to capture the global information of the decision boundary.  When determining the weight of a new support vector, we confine its influence only to its k-nearest opposite support vectors.  This can restrict the effect of new instances and prevent the harm of outliers.  More importantly, we design a sophisticated scheme to compensate the model after replacement is conducted when either buffer is full.  With this compensation, the learned model approaches the one learned with infinite budgets.  We present both theoretical analysis and extensive experimental comparison to demonstrate the effectiveness of our proposed KOIL.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SOSE</abbr>
    
  
  </div>

  <div id="gao2015ar" class="col-sm-8">
    
      <span class="title"><a href="https://ieeexplore.ieee.org/abstract/document/7133542">Ar-tracker: Track the dynamics of mobile apps via user review mining</a></span>
      <span class="author">
        
          
            
              
                
                  Cuiyun Gao,
                
              
            
          
        
          
            
              
                
                  Hui Xu,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  and Yangfan Zhou
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In 2015 IEEE Symposium on Service-Oriented System Engineering</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>User-generated reviews on mobile applications (apps) are a valuable source of data for developers to improve the quality of their apps. But the reviews are usually massive in size and span over multiple topics, thus leading to great challenges for developers to efficiently identify the key reviews of interest. In recent studies, automatic user review mining has been recognized as a key solution to address this challenge. The existing methods, however, require extensive human efforts to manually label the training data. Besides, they only analyze the static characteristics over the whole set of collected reviews, while ignoring the dynamic information embedded in the reviews of different time periods. In this paper, we propose ’AR-Tracker’, a new framework to mine user reviews without the need of human labeling and track the dynamics from the top-ranked reviews. Through extensive experiments on the reviews of four popular mobile apps collected over 7 months, we show that AR-Tracker can still achieve comparable accuracy with the state-of-the-art methods, e.g., AR-Miner. Additionally, a case study on Facebook reviews further validates the effectiveness of ’AR-Tracker’ in tracking the dynamics.</p>
    </span>
    
  </div>
</div>
</li></ol>

</div>

<div class="publications" id="pub-list-by-selected" style="display:none">
<ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="reid21emnlp" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2109.04715">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages</a></span>
      <span class="author">
        
          
            
              
                
                  Machel Reid,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Yutaka Matsuo
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/machelreid/afromt" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="ruder21emnlp" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/abs/2104.07412">XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation</a></span>
      <span class="author">
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Noah Constant,
                
              
            
          
        
          
            
              
                
                  Jan Botha,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  Jinlan Fu,
                
              
            
          
        
          
            
              
                
                  Pengfei Liu,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Dan Garrette,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  and Melvin Johnson
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/google-research/xtreme" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NAACL</abbr>
    
  
  </div>

  <div id="hu21explicit" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2010.07972.pdf">Explicit Alignment Objectives for Multilingual Bidirectional Encoders</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Melvin Johnson,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  and Graham Neubig
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</em>
      
      
        2021
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="http://github.com/junjiehu/amber" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="hu20icml" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2003.11080.pdf">XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation</a></span>
      <span class="author">
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Sebastian Ruder,
                
              
            
          
        
          
            
              
                
                  Aditya Siddhant,
                
              
            
          
        
          
            
              
                
                  Graham Neubig,
                
              
            
          
        
          
            
              
                
                  Orhan Firat,
                
              
            
          
        
          
            
              
                
                  and Melvin Johnson
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In International Conference on Machine Learning (ICML)</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/google-research/xtreme" target="_blank">Code</a>]
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.</p>
    </span>
    
  </div>
</div>
</li></ol>
</div>


<h3>Preprints</h3>
<div class="publications">

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="sreedhar2022" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2205.11490.pdf">Local Byte Fusion for Neural Machine Translation</a></span>
      <span class="author">
        
          
            
              
                
                  Makesh Narsimhan Sreedhar,
                
              
            
          
        
          
            
              
                
                  Xiangpeng Wan,
                
              
            
          
        
          
            
              
                
                  Yu Cheng,
                
              
            
          
        
          
            
              
                and <ins><strong>Junjie Hu</strong></ins>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:2205.11490</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="Dinh2022UtilizingLP" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/2205.11616.pdf">Utilizing Language-Image Pretraining for Efficient and Robust Bilingual Word Alignment</a></span>
      <span class="author">
        
          
            
              
                
                  Tuan Dinh,
                
              
            
          
        
          
            
              
                
                  Jy-yong Sohn,
                
              
            
          
        
          
            
              
                
                  Shashank Rajput,
                
              
            
          
        
          
            
              
                
                  Timothy Ossowski,
                
              
            
          
        
          
            
              
                
                  Yifei Ming,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Dimitris Papailiopoulos,
                
              
            
          
        
          
            
              
                
                  and Kangwook Lee
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:2205.11616</em>
      
      
        2022
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="chaudhary2019ariel" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/1902.08899.pdf">The ARIEL-CMU systems for LoReHLT18</a></span>
      <span class="author">
        
          
            
              
                
                  Aditi Chaudhary,
                
              
            
          
        
          
            
              
                
                  Siddharth Dalmia,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Xinjian Li,
                
              
            
          
        
          
            
              
                
                  Austin Matthews,
                
              
            
          
        
          
            
              
                
                  Aldrian Obaja Muis,
                
              
            
          
        
          
            
              
                
                  Naoki Otani,
                
              
            
          
        
          
            
              
                
                  Shruti Rijhwani,
                
              
            
          
        
          
            
              
                
                  Zaid Sheikh,
                
              
            
          
        
          
            
              
                
                  Nidhi Vyas,
                
              
            
          
        
          
            
              
                
                  and  others
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:1902.08899</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="zhao2017principled" class="col-sm-8">
    
      <span class="title"><a href="https://arxiv.org/pdf/1705.09011.pdf">Principled hybrids of generative and discriminative domain adaptation</a></span>
      <span class="author">
        
          
            
              
                
                  Han Zhao,
                
              
            
          
        
          
            
              
                
                  Zhenyao Zhu,
                
              
            
          
        
          
            
              
                <ins><strong>Junjie Hu</strong></ins>,
              
            
          
        
          
            
              
                
                  Adam Coates,
                
              
            
          
        
          
            
              
                
                  and Geoff Gordon
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:1705.09011</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

</div>


      
<div class="info-list">
  <h3 id="teaching" name="teaching">Teaching</h3>
  <ul>
    <li><p><a href="https://junjiehu.github.io/cs769-spring22/">CS 769 Advanced Natural Language Processing (Spring 2022)</a> </p></li>
    <!-- <li><p>Guest Lecture on Machine Translation in 11-411/611 Natural Language Processing, 2020 Spring, 2019 Spring, 2018 Fall.</p></li>
    <li><p>Guest Lecture on Multi-task Multi-lingual Learning Models in 11-747 Neural Networks for NLP, 2018 Spring.</p></li>
    <li><p>Teaching Assistant in 11-747 Neural Networks for NLP, 2018 Spring.</p></li>
    <li><p>Teaching Assistant in 11-731 Machine Translation and Sequence-to-sequence Models, 2018 Fall.</p></li>
    <li><p>Teaching Assistant in CSCI3100 Software Engineering, 2014 Spring, 2015 Spring.</p></li>
    <li><p>Teaching Assistant in CSCI3170 Introduction to Database System, 2013 Fall.</p></li>
    <li><p>Teaching Assistant in CSCI5250 Information Retrieval and Web Search, 2014 Fall.</p></li> -->
  </ul>

</div>


      <div class="info-list">
  <h3 id="talks" name="talks">Talks</h3>
  <ul>
    <li><p>Invited Talk at <a href="https://talks.cam.ac.uk/talk/index/175475">University of Cambridge, LTL Seminar</a>, June 09, 2022.</p></li>
    <li><p>Invited Talk at <a href="https://langsci.wisc.edu/2022/03/07/linguistics-fridays-spring-2022-schedule/">Lingustics Fridays Seminar at UW-Madison</a>, April 01, 2022.</p></li>
    <li><p>Invited Talk at <a href="https://www.microsoft.com/en-us/research/group/cognitive-services-research/talks/">Microsoft Azure Cognitive Services Research</a>, January 20, 2022.</p></li>
    <li><p>Invited Talk at <a href="https://www.meetup.com/Bay-Area-NLP/events/281277123/">Bay Area NLP Seminar</a>, November 18, 2021.</p></li>
    <li><p>Invited Talk at ICTR Seminar at UW-Madison, October 26, 2021.</p></li>
    <li><p>Invited Talk at Microsoft Research Summit, October 21, 2021.</p></li>
    <li><p>Invited Talk at CIBM Seminar at UW-Madison, October 19, 2021.</p></li>
    <li><p>Invited Talk at IFDS Ideas Forum at UW-Madison, October 11, 2021.</p></li>
    <li><p>XTREME: A Massively Multilingual Multi-task Benchmarkfor Evaluating Cross-lingual Generalization, Junjie Hu, LTI Summer Seminar Series at Carnegie Mellon University, Pittsburgh, July 2, 2020.</p></li>
    <li><p>Pre-training of Multilingual Encoder for Crosslingual Transfer, Junjie Hu, Google Translate Team, Mountain View, August 20 2019.</p></li>
    <li><p><a href="http://www.cs.cmu.edu/~aiseminar/abstract/19-04-30.html">Cross-Lingual and Cross Domain Transfer for Neural Machine Translation</a>, Junjie Hu, AI Seminar at Carnegie Mellon University, Pittsburgh April 30 2019.</p></li>
    <li><p>Transfer Learning for Multilingual Neural Machine Translation, Junjie Hu, SMART-Select Workshop on Multilingual Models and Unsupervised NMT supported by DG Connect of the European Commission, Luxembourg, June 20 2019. Facebook AI Research Lab, Paris, June 21 2019.</p></li>
    <li><p>Rethinking Visual Storytelling: What Makes A Good Story? Junjie Hu, Microsoft 365 AI Research, Redmond, August 23 2018.</p></li>
    <li><p>Machine Reading Comprehension via Structural Tree Embeddings, Junjie Hu, Seminar at Chinese University of Hong Kong, March 5 2018.</p></li>
    <li><p>Lorelei: Understanding Low Resource Languages, Pat Littell, Junjie Hu, Shruti Rijhwani, and Ruochen Xu. LTI Colloquium at Carnegie Mellon University, Pittsburgh, September 8, 2017.</p></li>
    <li><p>Natural Communication for Human-Robot Collaboration, Junjie Hu, Symposium on Natural Communication for Human-Robot Collaboration, November 9, 2017.</p></li>
  </ul>

</div>

      
<div class="info-list">
  <h3 id="awards" name="awards">Selected Awards and Scholarships</h3>
  <ul>
  	<li><p>CMU Graduate Student Assembly Dissertation Writing Group Grant, 2020</p></li>
  	<li><p>CMU Graduate Student Assembly Conference Travel Grant, 2020</p></li>
  	<li><p>NAACL 2019 Best Demonstration Paper Nomination, 2019</p></li>
    <li><p>Graduate Research Scholarship, Carnegie Mellon University, 2015-2021</p></li>
    <li><p>Postgraduate Scholarship, The Chinese University of Hong Kong, 2013-2015</p></li>
    <li><p>Certificate of Merit for Teaching Assistantship, Department of CSE, Chinese University of Hong Kong, 2013-2014</p></li>
    <li><p>IBM Outstanding Student Scholarship (1 of 77 winners in China), 2012-2013</p></li>
    <li><p>Outstanding Undergraduate Awards by China Computer Federation (99 winners), 2012-2013</p></li>
    <li><p>National Scholarship, the Ministry of Education, 2010-2011, 2011-2012</p></li>
  </ul>

</div>

    

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022 Junjie Hu
.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>. 

    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







<script type="text/javascript">
  function display(id) {
    var currentDisplay = document.getElementById(id).style.display;
    if (currentDisplay == 'none') {
        document.getElementById(id).style.display = "inline";
    }
    else {
        document.getElementById(id).style.display = "none";
    }
  }

  function onClickHandler(){
    display('pub-list-by-selected');
    display('pub-list-by-date')
  }
    // $("#pub-by-selected").on('click', function() {
    //   document.getElementById('pub-list-by-date').style.display = "none";
    //   document.getElementById('pub-list-by-selected').style.display = "inline";
    //   $('#pub-by-data').attr("style", "");
    //   $('#pub-by-selected').css({ "color": "#000000", "text-decoration": "underline" });
    // }

    // $("#pub-by-date").on('click', function() {
    //   document.getElementById('pub-list-by-date').style.display = "inline";
    //   document.getElementById('pub-list-by-selected').style.display = "none";
    //   $('#pub-by-selected').attr("style", "");
    //   $('#pub-by-date').css({ "color": "#000000", "text-decoration": "underline" });
    // }
</script>

</html>
